<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Custom Layers - keras-text Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  <link href="../css/extras.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Custom Layers";
    var mkdocs_page_input_path = "keras_text.models.layers.md";
    var mkdocs_page_url = "/keras_text.models.layers/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> keras-text Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">API Docs</span>
    <ul class="subnav">
                <li class=" current">
                    
    <span class="caption-text">Models</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../keras_text.models.sequence_encoders/">Sequence Processing Models</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../keras_text.models.token_model/">Sequence Model Builder Factory</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../keras_text.models.sentence_model/">Sentence Model Builder Factory</a>
                </li>
                <li class="toctree-l3 current">
                    
    <a class="current" href="./">Custom Layers</a>
    <ul class="subnav">
            
    <li class="toctree-l4"><a href="#attentionlayer">AttentionLayer</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#attentionlayerbuilt">AttentionLayer.built</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayerinput">AttentionLayer.input</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#attentionlayerinput_mask">AttentionLayer.input_mask</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_1">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_1">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#attentionlayerinput_shape">AttentionLayer.input_shape</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_2">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_2">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#attentionlayerlosses">AttentionLayer.losses</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayernon_trainable_weights">AttentionLayer.non_trainable_weights</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayeroutput">AttentionLayer.output</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_3">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_3">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#attentionlayeroutput_mask">AttentionLayer.output_mask</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_4">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_4">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#attentionlayeroutput_shape">AttentionLayer.output_shape</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_5">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_5">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#attentionlayertrainable_weights">AttentionLayer.trainable_weights</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayerupdates">AttentionLayer.updates</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayerweights">AttentionLayer.weights</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayer__init__">AttentionLayer.__init__</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayerbuild">AttentionLayer.build</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayercall">AttentionLayer.call</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayercompute_mask">AttentionLayer.compute_mask</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayercompute_output_shape">AttentionLayer.compute_output_shape</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayerget_attention_tensor">AttentionLayer.get_attention_tensor</a></li>
        
            <li><a class="toctree-l5" href="#attentionlayerget_config">AttentionLayer.get_config</a></li>
        
            <li><a class="toctree-l5" href="#consumemask">ConsumeMask</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_6">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_6">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#consumemaskinput_mask">ConsumeMask.input_mask</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_7">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_7">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#consumemaskinput_shape">ConsumeMask.input_shape</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_8">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_8">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#consumemasklosses">ConsumeMask.losses</a></li>
        
            <li><a class="toctree-l5" href="#consumemasknon_trainable_weights">ConsumeMask.non_trainable_weights</a></li>
        
            <li><a class="toctree-l5" href="#consumemaskoutput">ConsumeMask.output</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_9">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_9">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#consumemaskoutput_mask">ConsumeMask.output_mask</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_10">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_10">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#consumemaskoutput_shape">ConsumeMask.output_shape</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#returns_11">Returns</a></li>
    

    <li class="toctree-l4"><a href="#raises_11">Raises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#consumemasktrainable_weights">ConsumeMask.trainable_weights</a></li>
        
            <li><a class="toctree-l5" href="#consumemaskupdates">ConsumeMask.updates</a></li>
        
            <li><a class="toctree-l5" href="#consumemaskweights">ConsumeMask.weights</a></li>
        
            <li><a class="toctree-l5" href="#consumemaskcall">ConsumeMask.call</a></li>
        
            <li><a class="toctree-l5" href="#consumemaskcompute_mask">ConsumeMask.compute_mask</a></li>
        
        </ul>
    

    </ul>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">Data Processing</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../keras_text.processing/">Tokenizing and padding</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../keras_text.data/">Dataset management</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">Utilities</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../keras_text.generators/">Generators</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../keras_text.sampling/">Sampling</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../keras_text.utils/">Utils</a>
                </li>
    </ul>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">keras-text Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>API Docs &raquo;</li>
        
      
        
          <li>Models &raquo;</li>
        
      
    
    <li>Custom Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="http://github.com/raghakot/keras-text/blob/master/docs/templates/keras_text.models.layers.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p><strong>Source:</strong> <a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L0">keras_text/models/layers.py#L0</a></p>
<hr />
<h2 id="attentionlayer"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L35">AttentionLayer</a></h2>
<p>Attention layer that computes a learned attention over input sequence.</p>
<p>For details, see papers:
- https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf
- http://colinraffel.com/publications/iclr2016feed.pdf (fig 1)</p>
<p>Input:
 - <strong>x</strong>:  Input tensor of shape <code>(..., time_steps, features)</code> where <code>features</code> must be static (known).</p>
<p>Output:
2D tensor of shape <code>(..., features)</code>. i.e., <code>time_steps</code> axis is attended over and reduced.</p>
<h4 id="attentionlayerbuilt">AttentionLayer.built</h4>
<h4 id="attentionlayerinput">AttentionLayer.input</h4>
<p>Retrieves the input tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<h1 id="returns">Returns</h1>
<p>Input tensor or list of input tensors.</p>
<h1 id="raises">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="attentionlayerinput_mask">AttentionLayer.input_mask</h4>
<p>Retrieves the input mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<h1 id="returns_1">Returns</h1>
<p>Input mask tensor (potentially None) or list of input
  mask tensors.</p>
<h1 id="raises_1">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="attentionlayerinput_shape">AttentionLayer.input_shape</h4>
<p>Retrieves the input shape tuple(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<h1 id="returns_2">Returns</h1>
<p>Input shape tuple
  (or list of input shape tuples, one tuple per input tensor).</p>
<h1 id="raises_2">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="attentionlayerlosses">AttentionLayer.losses</h4>
<h4 id="attentionlayernon_trainable_weights">AttentionLayer.non_trainable_weights</h4>
<h4 id="attentionlayeroutput">AttentionLayer.output</h4>
<p>Retrieves the output tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<h1 id="returns_3">Returns</h1>
<p>Output tensor or list of output tensors.</p>
<h1 id="raises_3">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="attentionlayeroutput_mask">AttentionLayer.output_mask</h4>
<p>Retrieves the output mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<h1 id="returns_4">Returns</h1>
<p>Output mask tensor (potentially None) or list of output
  mask tensors.</p>
<h1 id="raises_4">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="attentionlayeroutput_shape">AttentionLayer.output_shape</h4>
<p>Retrieves the output shape tuple(s) of a layer.</p>
<p>Only applicable if the layer has one inbound node,
or if all inbound nodes have the same output shape.</p>
<h1 id="returns_5">Returns</h1>
<p>Output shape tuple
  (or list of input shape tuples, one tuple per output tensor).</p>
<h1 id="raises_5">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="attentionlayertrainable_weights">AttentionLayer.trainable_weights</h4>
<h4 id="attentionlayerupdates">AttentionLayer.updates</h4>
<h4 id="attentionlayerweights">AttentionLayer.weights</h4>
<hr />
<h3 id="attentionlayer__init__"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L49">AttentionLayer.<code>__init__</code></a></h3>
<pre><code class="python">__init__(self, kernel_initializer=&quot;he_normal&quot;, kernel_regularizer=None, kernel_constraint=None, \
    use_bias=True, bias_initializer=&quot;zeros&quot;, bias_regularizer=None, bias_constraint=None, \
    use_context=True, context_initializer=&quot;he_normal&quot;, context_regularizer=None, \
    context_constraint=None, attention_dims=None, **kwargs)
</code></pre>

<p><em>Args:</em></p>
<ul>
<li><strong>attention_dims</strong>:  The dimensionality of the inner attention calculating neural network.
  For input <code>(32, 10, 300)</code>, with <code>attention_dims</code> of 100, the output is <code>(32, 10, 100)</code>.
  i.e., the attended words are 100 dimensional. This is then collapsed via summation to
  <code>(32, 10, 1)</code> to indicate the attention weights for 10 words.
  If set to None, <code>features</code> dims are used as <code>attention_dims</code>. (Default value: None)</li>
</ul>
<hr />
<h3 id="attentionlayerbuild"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L92">AttentionLayer.build</a></h3>
<pre><code class="python">build(self, input_shape)
</code></pre>

<hr />
<h3 id="attentionlayercall"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L123">AttentionLayer.call</a></h3>
<pre><code class="python">call(self, x, mask=None)
</code></pre>

<hr />
<h3 id="attentionlayercompute_mask"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L146">AttentionLayer.compute_mask</a></h3>
<pre><code class="python">compute_mask(self, input, input_mask=None)
</code></pre>

<hr />
<h3 id="attentionlayercompute_output_shape"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L150">AttentionLayer.compute_output_shape</a></h3>
<pre><code class="python">compute_output_shape(self, input_shape)
</code></pre>

<hr />
<h3 id="attentionlayerget_attention_tensor"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L153">AttentionLayer.get_attention_tensor</a></h3>
<pre><code class="python">get_attention_tensor(self)
</code></pre>

<hr />
<h3 id="attentionlayerget_config"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L158">AttentionLayer.get_config</a></h3>
<pre><code class="python">get_config(self)
</code></pre>

<hr />
<h2 id="consumemask"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L174">ConsumeMask</a></h2>
<p>Layer that prevents mask propagation.</p>
<h4 id="consumemaskbuilt">ConsumeMask.built</h4>
<h4 id="consumemaskinput">ConsumeMask.input</h4>
<p>Retrieves the input tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<h1 id="returns_6">Returns</h1>
<p>Input tensor or list of input tensors.</p>
<h1 id="raises_6">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="consumemaskinput_mask">ConsumeMask.input_mask</h4>
<p>Retrieves the input mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<h1 id="returns_7">Returns</h1>
<p>Input mask tensor (potentially None) or list of input
  mask tensors.</p>
<h1 id="raises_7">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="consumemaskinput_shape">ConsumeMask.input_shape</h4>
<p>Retrieves the input shape tuple(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<h1 id="returns_8">Returns</h1>
<p>Input shape tuple
  (or list of input shape tuples, one tuple per input tensor).</p>
<h1 id="raises_8">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="consumemasklosses">ConsumeMask.losses</h4>
<h4 id="consumemasknon_trainable_weights">ConsumeMask.non_trainable_weights</h4>
<h4 id="consumemaskoutput">ConsumeMask.output</h4>
<p>Retrieves the output tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<h1 id="returns_9">Returns</h1>
<p>Output tensor or list of output tensors.</p>
<h1 id="raises_9">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="consumemaskoutput_mask">ConsumeMask.output_mask</h4>
<p>Retrieves the output mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<h1 id="returns_10">Returns</h1>
<p>Output mask tensor (potentially None) or list of output
  mask tensors.</p>
<h1 id="raises_10">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="consumemaskoutput_shape">ConsumeMask.output_shape</h4>
<p>Retrieves the output shape tuple(s) of a layer.</p>
<p>Only applicable if the layer has one inbound node,
or if all inbound nodes have the same output shape.</p>
<h1 id="returns_11">Returns</h1>
<p>Output shape tuple
  (or list of input shape tuples, one tuple per output tensor).</p>
<h1 id="raises_11">Raises</h1>
<ul>
<li><strong>AttributeError</strong>:  if the layer is connected to
more than one incoming layers.</li>
</ul>
<h4 id="consumemasktrainable_weights">ConsumeMask.trainable_weights</h4>
<h4 id="consumemaskupdates">ConsumeMask.updates</h4>
<h4 id="consumemaskweights">ConsumeMask.weights</h4>
<hr />
<h3 id="consumemaskcall"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L182">ConsumeMask.call</a></h3>
<pre><code class="python">call(self, x, mask=None)
</code></pre>

<hr />
<h3 id="consumemaskcompute_mask"><a href="https://github.com/raghakot/keras-text/tree/master/keras_text/models/layers.py#L178">ConsumeMask.compute_mask</a></h3>
<pre><code class="python">compute_mask(self, input, input_mask=None)
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../keras_text.processing/" class="btn btn-neutral float-right" title="Tokenizing and padding">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../keras_text.models.sentence_model/" class="btn btn-neutral" title="Sentence Model Builder Factory"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="http://github.com/raghakot/keras-text/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../keras_text.models.sentence_model/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../keras_text.processing/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="../search/require.js"></script>
      <script src="../search/search.js"></script>

</body>
</html>
